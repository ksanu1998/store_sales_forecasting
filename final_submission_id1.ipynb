{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Importing libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core import display as ICD"
   ],
   "metadata": {
    "_uuid": "2421d6ba-cdb2-4d3b-8284-da7324312501",
    "_cell_guid": "51c84c49-7485-40db-af1a-b798ae0c2cea",
    "collapsed": false,
    "papermill": {
     "duration": 0.03131,
     "end_time": "2022-11-26T04:30:01.080042",
     "exception": false,
     "start_time": "2022-11-26T04:30:01.048732",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:41.891733Z",
     "iopub.execute_input": "2022-12-14T01:57:41.892249Z",
     "iopub.status.idle": "2022-12-14T01:57:41.922780Z",
     "shell.execute_reply.started": "2022-12-14T01:57:41.892146Z",
     "shell.execute_reply": "2022-12-14T01:57:41.921753Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Supress some warnings for better clarity\n",
    "# Set some options also\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.reset_option(\"mode.chained_assignment\")"
   ],
   "metadata": {
    "_uuid": "743dd8d4-67c6-47ad-ba6f-0ca7d43aaaaa",
    "_cell_guid": "83fe7715-fa82-4d89-b2cd-c5677f6a7858",
    "collapsed": false,
    "papermill": {
     "duration": 0.099168,
     "end_time": "2022-11-26T04:30:01.19224",
     "exception": false,
     "start_time": "2022-11-26T04:30:01.093072",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:41.925771Z",
     "iopub.execute_input": "2022-12-14T01:57:41.926579Z",
     "iopub.status.idle": "2022-12-14T01:57:41.988053Z",
     "shell.execute_reply.started": "2022-12-14T01:57:41.926520Z",
     "shell.execute_reply": "2022-12-14T01:57:41.986904Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Variable to determine if items should be plotted\n",
    "# Warning: if true, the notebook may run out of memory due to the plots\n",
    "# Only set if interested in plotting\n",
    "plot_timeseries = False\n",
    "plot_importances = False"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:41.989569Z",
     "iopub.execute_input": "2022-12-14T01:57:41.990012Z",
     "iopub.status.idle": "2022-12-14T01:57:42.001198Z",
     "shell.execute_reply.started": "2022-12-14T01:57:41.989979Z",
     "shell.execute_reply": "2022-12-14T01:57:42.000134Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a dictionary in order to keep all the datasets in one place\n",
    "datasets = {}"
   ],
   "metadata": {
    "_uuid": "dc5bc621-ecf8-4de0-a0f5-e1a5c0869f06",
    "_cell_guid": "ec87517d-02e0-4d98-add7-077e0c9726fb",
    "collapsed": false,
    "papermill": {
     "duration": 0.022517,
     "end_time": "2022-11-26T04:30:17.352328",
     "exception": false,
     "start_time": "2022-11-26T04:30:17.329811",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:42.003674Z",
     "iopub.execute_input": "2022-12-14T01:57:42.004334Z",
     "iopub.status.idle": "2022-12-14T01:57:42.015791Z",
     "shell.execute_reply.started": "2022-12-14T01:57:42.004295Z",
     "shell.execute_reply": "2022-12-14T01:57:42.014392Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe for sales\n",
    "sales_df = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv',\n",
    "                          dtype={\n",
    "                            'store_nbr': 'category',\n",
    "                            'family': 'category',\n",
    "                            'onpromotion': 'uint32'\n",
    "                            },\n",
    "                         parse_dates=['date'],\n",
    "                         date_parser= lambda x: pd.to_datetime(x, format='%Y-%m-%d').to_period('D'),\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col=['date', 'store_nbr', 'family'],\n",
    "                         usecols = ['date', 'store_nbr', 'family','onpromotion', 'sales'])\n",
    "datasets['sales_init'] = sales_df\n",
    "datasets['sales_init'].head(5)"
   ],
   "metadata": {
    "_uuid": "829764f2-4f21-4755-8adb-c23fc03d926a",
    "_cell_guid": "bb1414b0-5764-4d6a-bde8-8ade82b47ed5",
    "collapsed": false,
    "papermill": {
     "duration": 3.802724,
     "end_time": "2022-11-26T04:30:21.169831",
     "exception": false,
     "start_time": "2022-11-26T04:30:17.367107",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:42.018035Z",
     "iopub.execute_input": "2022-12-14T01:57:42.018533Z",
     "iopub.status.idle": "2022-12-14T01:57:46.079646Z",
     "shell.execute_reply.started": "2022-12-14T01:57:42.018443Z",
     "shell.execute_reply": "2022-12-14T01:57:46.078612Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe for test\n",
    "test_df = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv',\n",
    "                          dtype={\n",
    "                            'store_nbr': 'category',\n",
    "                            'family': 'category',\n",
    "                            'onpromotion': 'uint32'\n",
    "                            },\n",
    "                         parse_dates=['date'],\n",
    "                         date_parser= lambda x: pd.to_datetime(x, format='%Y-%m-%d').to_period('D'),\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col=['date', 'store_nbr', 'family'],\n",
    "                         usecols = ['date', 'store_nbr', 'family','onpromotion'])\n",
    "datasets['test_init'] = test_df\n",
    "datasets['test_init'].head(5)"
   ],
   "metadata": {
    "_uuid": "5f4b4ced-d03d-4ef0-8ebf-66d16adf9024",
    "_cell_guid": "d68cf70b-9adc-4db3-b1ba-1b267afadd47",
    "collapsed": false,
    "papermill": {
     "duration": 0.074371,
     "end_time": "2022-11-26T04:30:21.258441",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.18407",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.080980Z",
     "iopub.execute_input": "2022-12-14T01:57:46.081395Z",
     "iopub.status.idle": "2022-12-14T01:57:46.145144Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.081362Z",
     "shell.execute_reply": "2022-12-14T01:57:46.143948Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe for holidays\n",
    "holidays_df = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv',\n",
    "                           dtype={\n",
    "                             # Used str insted of category\n",
    "                             'type': 'str',\n",
    "                             'locale': 'str',\n",
    "                             'locale_name': 'str',\n",
    "                             'transferred': 'boolean'\n",
    "                             },\n",
    "                          parse_dates=['date'],\n",
    "                          date_parser= lambda x: pd.to_datetime(x, format='%Y-%m-%d').to_period('D'),\n",
    "                          infer_datetime_format=True,\n",
    "                          index_col=['date'],\n",
    "                          usecols = ['date', 'type', 'locale','locale_name','transferred'])\n",
    "datasets['holidays_init'] = holidays_df\n",
    "datasets['holidays_init'].head(5)"
   ],
   "metadata": {
    "_uuid": "e5e99ea2-8a85-46e0-8ac5-83bf0078eaf0",
    "_cell_guid": "5462d2f3-85e6-43fc-a0c4-f3a7b89c66d7",
    "collapsed": false,
    "papermill": {
     "duration": 0.036583,
     "end_time": "2022-11-26T04:30:21.309523",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.27294",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.146491Z",
     "iopub.execute_input": "2022-12-14T01:57:46.147189Z",
     "iopub.status.idle": "2022-12-14T01:57:46.170362Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.147153Z",
     "shell.execute_reply": "2022-12-14T01:57:46.168976Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe for oil\n",
    "oil_df = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv',\n",
    "                     parse_dates=['date'],\n",
    "                     date_parser= lambda x: pd.to_datetime(x, format='%Y-%m-%d').to_period('D'),\n",
    "                     infer_datetime_format=True,\n",
    "                     index_col=['date'])\n",
    "datasets['oil_init'] = oil_df\n",
    "datasets['oil_init'].head(5)"
   ],
   "metadata": {
    "_uuid": "17d7b1f2-914a-432b-934a-9411ef6d82cb",
    "_cell_guid": "9d08a28a-503c-432c-8f4b-875122118fdc",
    "collapsed": false,
    "papermill": {
     "duration": 0.032068,
     "end_time": "2022-11-26T04:30:21.356712",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.324644",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.171387Z",
     "iopub.execute_input": "2022-12-14T01:57:46.172024Z",
     "iopub.status.idle": "2022-12-14T01:57:46.192743Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.171992Z",
     "shell.execute_reply": "2022-12-14T01:57:46.191546Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe for transactions\n",
    "transactions_df = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv',\n",
    "                          dtype={\n",
    "                            'store_nbr': 'category',\n",
    "                            'transactions': 'int'\n",
    "                            },\n",
    "                         parse_dates=['date'],\n",
    "                         date_parser= lambda x: pd.to_datetime(x, format='%Y-%m-%d').to_period('D'),\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col=['date', 'store_nbr'],\n",
    "                         usecols = ['date', 'store_nbr', 'transactions'])\n",
    "datasets['transactions_init'] = transactions_df\n",
    "datasets['transactions_init'].head(5)"
   ],
   "metadata": {
    "_uuid": "f60208de-58ba-4f51-b50c-0cfe33c2cd89",
    "_cell_guid": "227ef394-c1fa-45cf-bcd2-77e4b1cfa32d",
    "collapsed": false,
    "papermill": {
     "duration": 0.123234,
     "end_time": "2022-11-26T04:30:21.494687",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.371453",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.194299Z",
     "iopub.execute_input": "2022-12-14T01:57:46.194633Z",
     "iopub.status.idle": "2022-12-14T01:57:46.308566Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.194603Z",
     "shell.execute_reply": "2022-12-14T01:57:46.307333Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataframe for stores\n",
    "stores_df = pd.read_csv('../input/store-sales-time-series-forecasting/stores.csv',\n",
    "                          dtype={\n",
    "                            'store_nbr': 'category',\n",
    "                            'city':  'category',\n",
    "                            'state':  'category',\n",
    "                            'cluster':  'category',\n",
    "                            },\n",
    "                         usecols = ['store_nbr', 'city', 'state', 'cluster', 'type'])\n",
    "datasets['stores_init'] = stores_df\n",
    "datasets['stores_init'].head(5)"
   ],
   "metadata": {
    "_uuid": "40232a8e-0158-4c22-89a5-739ca03c83ff",
    "_cell_guid": "b45d89b0-7a25-4863-b788-98d151a49ee2",
    "collapsed": false,
    "papermill": {
     "duration": 0.033143,
     "end_time": "2022-11-26T04:30:21.543006",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.509863",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.314553Z",
     "iopub.execute_input": "2022-12-14T01:57:46.314970Z",
     "iopub.status.idle": "2022-12-14T01:57:46.339708Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.314927Z",
     "shell.execute_reply": "2022-12-14T01:57:46.338799Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# find the size of all dataframes combined\n",
    "all_df_size_mb = sum([sys.getsizeof(df) for df in datasets.values()]) / 10**6\n",
    "print(f'All dataframes reserve {all_df_size_mb} MB')"
   ],
   "metadata": {
    "_uuid": "8d4f0a23-b9f5-4780-b35b-e4461d6e107b",
    "_cell_guid": "fa3c2885-bdd5-4844-85f3-87647b5676a7",
    "collapsed": false,
    "papermill": {
     "duration": 0.150593,
     "end_time": "2022-11-26T04:30:21.736187",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.585594",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.340756Z",
     "iopub.execute_input": "2022-12-14T01:57:46.341527Z",
     "iopub.status.idle": "2022-12-14T01:57:46.469469Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.341491Z",
     "shell.execute_reply": "2022-12-14T01:57:46.468268Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# method to preview dataframe with better clarity\n",
    "def dataset_preview(df, head=3):\n",
    "    print('='*50)\n",
    "    print('Preview:')\n",
    "    ICD.display(df.head(head))\n",
    "    print('*'*50)\n",
    "    print('Describe:')\n",
    "    ICD.display(df.describe())\n",
    "    print('*'*50)\n",
    "    print('Info:')\n",
    "    print(df.info())\n",
    "    print('='*50)\n",
    "    \n",
    "# method to create lag features\n",
    "def create_lags(df, target_col, lags=1, suffix='_lag', drop_target=True):\n",
    "    for i in range(1, lags + 1) :\n",
    "        df[f'{target_col}({suffix}{i})'] = df[target_col].shift(i)\n",
    "    if drop_target:\n",
    "        df = df.drop(columns=[target_col])\n",
    "    return df"
   ],
   "metadata": {
    "_uuid": "71fd7537-d45b-4466-8da1-d2865e0dcd5a",
    "_cell_guid": "09e00500-553d-4fcc-bfd1-912ac0042f96",
    "collapsed": false,
    "papermill": {
     "duration": 0.027049,
     "end_time": "2022-11-26T04:30:21.838358",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.811309",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.470701Z",
     "iopub.execute_input": "2022-12-14T01:57:46.471038Z",
     "iopub.status.idle": "2022-12-14T01:57:46.480361Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.471006Z",
     "shell.execute_reply": "2022-12-14T01:57:46.479359Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# cherry-picking stores based on some peculiarities and adding new corresponding features\n",
    "stores_df = datasets['stores_init'].copy()\n",
    "# add features based on Stores origins\n",
    "#  - areas with only one store\n",
    "stores_df['uniquestore'] = stores_df['city'].apply(lambda x: 0 if x in ['Quito', 'Guayaquil', 'Santo Domingo', 'Cuenca', 'Manta', 'Machala', 'Latacunga', 'Ambato'] else 1)\n",
    "# - stores which were inaugurated in train/test period\n",
    "stores_df['newstore'] = stores_df['store_nbr'].apply(lambda x: 1 if x in [19, 20, 21, 28, 35, 41, 51, 52] else 0)\n",
    "stores_df = stores_df.rename(columns={'type' : 'store'}) \n",
    "stores_df = stores_df.set_index('store_nbr')"
   ],
   "metadata": {
    "_uuid": "a234aa44-79ae-452e-b048-c440d0ebd66a",
    "_cell_guid": "b168fa1a-c99f-4d4c-8356-c15ee05ea317",
    "collapsed": false,
    "papermill": {
     "duration": 0.031811,
     "end_time": "2022-11-26T04:30:21.885225",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.853414",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.481489Z",
     "iopub.execute_input": "2022-12-14T01:57:46.482326Z",
     "iopub.status.idle": "2022-12-14T01:57:46.503126Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.482232Z",
     "shell.execute_reply": "2022-12-14T01:57:46.501970Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add the dataframe with new Stores features to the dictionary\n",
    "datasets['stores_trans'] = stores_df"
   ],
   "metadata": {
    "_uuid": "185333f5-dc5b-4353-968e-4afd15cbe998",
    "_cell_guid": "173af87c-921b-439b-84eb-100149af4f30",
    "collapsed": false,
    "papermill": {
     "duration": 0.025684,
     "end_time": "2022-11-26T04:30:21.92609",
     "exception": false,
     "start_time": "2022-11-26T04:30:21.900406",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.506186Z",
     "iopub.execute_input": "2022-12-14T01:57:46.506582Z",
     "iopub.status.idle": "2022-12-14T01:57:46.515441Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.506549Z",
     "shell.execute_reply": "2022-12-14T01:57:46.514337Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# preview the holidays dataset\n",
    "dataset_preview(datasets['holidays_init'])"
   ],
   "metadata": {
    "_uuid": "1c3b3635-4ccb-459d-8464-1e869ed21e0a",
    "_cell_guid": "c9d462c0-28d2-4fa3-b9ac-30be6ac8290a",
    "collapsed": false,
    "papermill": {
     "duration": 0.064181,
     "end_time": "2022-11-26T04:30:22.093104",
     "exception": false,
     "start_time": "2022-11-26T04:30:22.028923",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.517026Z",
     "iopub.execute_input": "2022-12-14T01:57:46.517435Z",
     "iopub.status.idle": "2022-12-14T01:57:46.562350Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.517398Z",
     "shell.execute_reply": "2022-12-14T01:57:46.561051Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a series object for range of dates (training data + test data) from 2013-01-01 to 2017-08-31\n",
    "total_days_range = pd.Series(pd.date_range('2013-01-01', '2017-08-31').to_period('D'), name='date')\n",
    "\n",
    "# create a dataframe out of the series object\n",
    "calendar_df = pd.DataFrame(index=total_days_range)\n",
    "\n",
    "# merge the calendar dataframe with the holidays dataset\n",
    "calendar_df = calendar_df.merge(datasets['holidays_init'], how='left',on=['date'])\n",
    "calendar_df.head()"
   ],
   "metadata": {
    "_uuid": "26ea2c8a-a7aa-4537-96e8-2eba7724a14e",
    "_cell_guid": "66ae0b06-9d99-4d79-a790-8d64eb64b89a",
    "collapsed": false,
    "papermill": {
     "duration": 0.045174,
     "end_time": "2022-11-26T04:30:22.154203",
     "exception": false,
     "start_time": "2022-11-26T04:30:22.109029",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.564194Z",
     "iopub.execute_input": "2022-12-14T01:57:46.564961Z",
     "iopub.status.idle": "2022-12-14T01:57:46.590190Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.564911Z",
     "shell.execute_reply": "2022-12-14T01:57:46.588790Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# map the missing rows as follows: 'type' - 'Work Day', 'locale' - 'National', 'locale_name' - 'Ecuador', 'transferred' - False\n",
    "calendar_df.loc[calendar_df.isna().any(axis=1), ['type', 'locale', 'locale_name', 'transferred']] = 'Work Day', 'National', 'Ecuador', False\n",
    "calendar_df.head()"
   ],
   "metadata": {
    "_uuid": "2b2b405f-ce24-485d-adc8-0fa427f89317",
    "_cell_guid": "6c75c47a-c92e-4355-80c1-0024fdfb24aa",
    "collapsed": false,
    "papermill": {
     "duration": 0.037242,
     "end_time": "2022-11-26T04:30:22.237359",
     "exception": false,
     "start_time": "2022-11-26T04:30:22.200117",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.592149Z",
     "iopub.execute_input": "2022-12-14T01:57:46.592627Z",
     "iopub.status.idle": "2022-12-14T01:57:46.610657Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.592581Z",
     "shell.execute_reply": "2022-12-14T01:57:46.609451Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# apply corrections based on holidays\n",
    "\n",
    "# different holiday types are:\n",
    "# * Work Day\n",
    "# * Bridge\n",
    "# * Transfer\n",
    "# * Event\n",
    "# * Holiday\n",
    "# * Additional\n",
    "\n",
    "# initialize all the days as working\n",
    "# * if the day is a working day, set wd = True\n",
    "# * if the day is not a working day, set wd = False\n",
    "\n",
    "easter_dates = ['2017-04-16', '2016-03-27', '2015-04-05', '2014-04-20', '2013-03-31']\n",
    "easter_dates = [pd.to_datetime(date, format='%Y-%m-%d').to_period('D') for date in easter_dates]\n",
    "\n",
    "calendar_df['wd'] = True\n",
    "calendar_df.loc[calendar_df.type == 'Bridge'  , 'wd'] = False\n",
    "calendar_df.loc[calendar_df.type == 'Transfer', 'wd'] = False\n",
    "calendar_df.loc[(calendar_df.type == 'Additional') & (calendar_df.transferred == False), 'wd'] = False\n",
    "calendar_df.loc[(calendar_df.type == 'Holiday') & (calendar_df.transferred == False), 'wd'] = False\n",
    "calendar_df.loc[calendar_df.index.get_level_values('date').isin(easter_dates), 'wd'] = False\n",
    "\n",
    "calendar_df['isevent'] = False\n",
    "calendar_df.loc[calendar_df.type == 'Event'  , 'isevent'] = True\n",
    "calendar_df.loc[calendar_df.index.get_level_values('date').isin(easter_dates), 'isevent'] = True\n",
    "\n",
    "calendar_df = pd.get_dummies(calendar_df, columns=['type'])\n",
    "calendar_df = pd.get_dummies(calendar_df, columns=['locale'])\n",
    "\n",
    "calendar_df.drop(columns=['locale_name', 'transferred'], inplace = True) \n",
    "\n",
    "# aggregate duplicate/redundant rows\n",
    "calendar_df = calendar_df.groupby('date').agg(lambda x: np.bitwise_or.reduce(x.values))\n",
    "\n",
    "calendar_df.head()"
   ],
   "metadata": {
    "_uuid": "b810e302-7f2f-469a-bd27-78132546eb62",
    "_cell_guid": "af8b6c54-ba62-4ff2-bc0d-a95be3b1bbb0",
    "collapsed": false,
    "papermill": {
     "duration": 0.554146,
     "end_time": "2022-11-26T04:30:22.807348",
     "exception": false,
     "start_time": "2022-11-26T04:30:22.253202",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:46.612247Z",
     "iopub.execute_input": "2022-12-14T01:57:46.612693Z",
     "iopub.status.idle": "2022-12-14T01:57:47.171457Z",
     "shell.execute_reply.started": "2022-12-14T01:57:46.612659Z",
     "shell.execute_reply": "2022-12-14T01:57:47.170065Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add the calendar dataframe to the dictionary\n",
    "datasets['calendar'] = calendar_df"
   ],
   "metadata": {
    "_uuid": "e2a17483-12cc-4965-8ba8-48f56a96d730",
    "_cell_guid": "ef386b8b-2c45-4b5f-8ba5-3d899d8b0647",
    "collapsed": false,
    "papermill": {
     "duration": 0.024809,
     "end_time": "2022-11-26T04:30:22.879379",
     "exception": false,
     "start_time": "2022-11-26T04:30:22.85457",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:47.173185Z",
     "iopub.execute_input": "2022-12-14T01:57:47.174454Z",
     "iopub.status.idle": "2022-12-14T01:57:47.180882Z",
     "shell.execute_reply.started": "2022-12-14T01:57:47.174401Z",
     "shell.execute_reply": "2022-12-14T01:57:47.178973Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# preview oil dataset\n",
    "dataset_preview(datasets['oil_init'])"
   ],
   "metadata": {
    "_uuid": "18d6dc7e-0622-49ff-97b3-b1cfdbb67c4e",
    "_cell_guid": "94956ae8-82ca-49aa-92b1-1caaf8c7f092",
    "collapsed": false,
    "papermill": {
     "duration": 0.054665,
     "end_time": "2022-11-26T04:30:23.013009",
     "exception": false,
     "start_time": "2022-11-26T04:30:22.958344",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:47.182892Z",
     "iopub.execute_input": "2022-12-14T01:57:47.184259Z",
     "iopub.status.idle": "2022-12-14T01:57:47.225645Z",
     "shell.execute_reply.started": "2022-12-14T01:57:47.184206Z",
     "shell.execute_reply": "2022-12-14T01:57:47.224338Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "oil_df = datasets['oil_init'].copy()\n",
    "\n",
    "# interpolate missing values\n",
    "oil_df = oil_df.resample('D').mean().interpolate(limit_direction='backward').reset_index()\n",
    "\n",
    "# add 4-day lags\n",
    "oil_df = create_lags(oil_df, target_col='dcoilwtico', lags=4, suffix='_lag', drop_target=False)\n",
    "# add weekly averaged values as a new feature\n",
    "oil_df['oil_week_avg'] = oil_df['dcoilwtico'].rolling(7).mean()\n",
    "# add biweekly averaged values as a new feature\n",
    "oil_df['oil_biweek_avg'] = oil_df['dcoilwtico'].rolling(14).mean()\n",
    "# add monthly averaged values as a new feature\n",
    "oil_df['oil_1_month_avg'] = oil_df['dcoilwtico'].rolling(30).mean()\n",
    "# add bimonthly averaged values as a new feature\n",
    "oil_df['oil_2_month_avg'] = oil_df['dcoilwtico'].rolling(60).mean()\n",
    "\n",
    "# drop NaN rows\n",
    "oil_df.dropna(inplace = True)\n",
    "\n",
    "# set index to date\n",
    "oil_df = oil_df.set_index('date')\n",
    "oil_df.head()"
   ],
   "metadata": {
    "_uuid": "f3ba38cc-8dd9-44fd-872c-45a17b12a188",
    "_cell_guid": "32a8d3c6-f35f-41fb-9ef1-6380dfc3e643",
    "collapsed": false,
    "papermill": {
     "duration": 0.063119,
     "end_time": "2022-11-26T04:30:23.092306",
     "exception": false,
     "start_time": "2022-11-26T04:30:23.029187",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:47.227972Z",
     "iopub.execute_input": "2022-12-14T01:57:47.228855Z",
     "iopub.status.idle": "2022-12-14T01:57:47.279701Z",
     "shell.execute_reply.started": "2022-12-14T01:57:47.228805Z",
     "shell.execute_reply": "2022-12-14T01:57:47.278338Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add the oil dataframe to the dictionary\n",
    "datasets['oil_trans'] = oil_df"
   ],
   "metadata": {
    "_uuid": "d27c09db-0cea-4695-ad60-7b2dd4ed9722",
    "_cell_guid": "5820d640-edb9-4932-967a-db9f945eeae9",
    "collapsed": false,
    "papermill": {
     "duration": 0.025488,
     "end_time": "2022-11-26T04:30:23.134236",
     "exception": false,
     "start_time": "2022-11-26T04:30:23.108748",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:47.281862Z",
     "iopub.execute_input": "2022-12-14T01:57:47.282778Z",
     "iopub.status.idle": "2022-12-14T01:57:47.289099Z",
     "shell.execute_reply.started": "2022-12-14T01:57:47.282719Z",
     "shell.execute_reply": "2022-12-14T01:57:47.287206Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create a feature dataframe\n",
    "feature_df = datasets['sales_init'].copy()\n",
    "test_df = datasets['test_init'].copy()\n",
    "test_df['sales'] = np.nan\n",
    "\n",
    "feature_df = pd.concat([feature_df, test_df], axis=0)\n",
    "feature_df.head()"
   ],
   "metadata": {
    "_uuid": "69950046-0395-41dc-91ac-4a3e019b2fa7",
    "_cell_guid": "024d0bb6-fe9d-401a-9141-547993a7c8e1",
    "collapsed": false,
    "papermill": {
     "duration": 0.184619,
     "end_time": "2022-11-26T04:30:23.403095",
     "exception": false,
     "start_time": "2022-11-26T04:30:23.218476",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:47.290383Z",
     "iopub.execute_input": "2022-12-14T01:57:47.290946Z",
     "iopub.status.idle": "2022-12-14T01:57:47.489750Z",
     "shell.execute_reply.started": "2022-12-14T01:57:47.290910Z",
     "shell.execute_reply": "2022-12-14T01:57:47.488461Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add store features to this dataframe\n",
    "feature_df = feature_df.merge(datasets['stores_trans'],  how='left',  left_index=True,  right_index=True)\n",
    "feature_df.drop(feature_df.filter(regex='_y$').columns, axis=1, inplace=True) \n",
    "\n",
    "# add a separate feature for stores that are closed\n",
    "feature_df['isclosed'] = feature_df.groupby(by=['date', 'store_nbr'])['sales'].transform(lambda x: 1 if x.sum()==0 else 0)\n",
    "feature_df.loc[((feature_df.index.get_level_values('date').year==2017) & (feature_df.index.get_level_values('date').month==8) & (feature_df.index.get_level_values('date').day>=16)) , 'isclosed'] = feature_df['isclosed'].apply(lambda x: 0) \n",
    "\n",
    "# merge calendar feature with this dataframe\n",
    "feature_df = feature_df.merge(datasets['calendar'], how='left',  left_index=True, right_index=True, suffixes=('', '_y'))\n",
    "\n",
    "# drop common columns\n",
    "feature_df.drop(feature_df.filter(regex='_y$').columns, axis=1, inplace=True)\n",
    "\n",
    "# add oil features to this dataframe\n",
    "# lags and 7-days moving average\n",
    "feature_df = feature_df.merge(datasets['oil_trans'], how='left',  left_index=True, right_index=True)\n",
    "feature_df.head()"
   ],
   "metadata": {
    "_uuid": "94e8cd8a-8b3a-47fd-b79e-089a07a2398e",
    "_cell_guid": "64953926-088a-403b-bb23-eb9ee754b0ef",
    "collapsed": false,
    "papermill": {
     "duration": 46.000053,
     "end_time": "2022-11-26T04:31:09.419824",
     "exception": false,
     "start_time": "2022-11-26T04:30:23.419771",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:57:47.491203Z",
     "iopub.execute_input": "2022-12-14T01:57:47.491646Z",
     "iopub.status.idle": "2022-12-14T01:58:33.737368Z",
     "shell.execute_reply.started": "2022-12-14T01:57:47.491612Z",
     "shell.execute_reply": "2022-12-14T01:58:33.736381Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot a yearly, monthly, and weekly plot for each family averaged over stores\n",
    "# Provides insight into some time related features\n",
    "# Create a dataframe of average sales per family per day, with year, day, and week info\n",
    "if plot_timeseries:\n",
    "    avg_family_df = datasets['sales_init'].copy()\n",
    "    avg_family_df = avg_family_df.reset_index().groupby(['date', 'family']).sales.mean()\n",
    "    avg_family_df = avg_family_df.reset_index().set_index('date')\n",
    "    avg_family_df['year'] = avg_family_df.index.year\n",
    "    avg_family_df['dayofyear'] = avg_family_df.index.dayofyear\n",
    "    avg_family_df['month'] = avg_family_df.index.month\n",
    "    avg_family_df['dayofmonth'] = avg_family_df.index.day\n",
    "    avg_family_df['week'] = avg_family_df.index.week\n",
    "    avg_family_df['dayofweek'] = avg_family_df.index.dayofweek"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:33.738626Z",
     "iopub.execute_input": "2022-12-14T01:58:33.739390Z",
     "iopub.status.idle": "2022-12-14T01:58:33.747022Z",
     "shell.execute_reply.started": "2022-12-14T01:58:33.739352Z",
     "shell.execute_reply": "2022-12-14T01:58:33.745883Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot average sales per family per year\n",
    "if plot_timeseries:\n",
    "    for family in avg_family_df.family.unique():\n",
    "        for year in avg_family_df.year.unique():\n",
    "            data = avg_family_df.loc[(avg_family_df.family == family) &\n",
    "                                    (avg_family_df.year == year), 'sales']\n",
    "            plt.plot(range(0, len(data)), data, label=year)\n",
    "        plt.legend()\n",
    "        plt.suptitle(family)\n",
    "        plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:33.748501Z",
     "iopub.execute_input": "2022-12-14T01:58:33.748862Z",
     "iopub.status.idle": "2022-12-14T01:58:33.779796Z",
     "shell.execute_reply.started": "2022-12-14T01:58:33.748826Z",
     "shell.execute_reply": "2022-12-14T01:58:33.778135Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the sales information over the months\n",
    "if plot_timeseries:\n",
    "    over_the_months_df = avg_family_df.groupby(['month', 'dayofmonth', 'family']).sales.mean().reset_index()\n",
    "    for family in over_the_months_df.family.unique():\n",
    "        for month in over_the_months_df.month.unique():\n",
    "            data = over_the_months_df.loc[(over_the_months_df.month == month) &\n",
    "                                         (over_the_months_df.family == family), 'sales']\n",
    "            plt.plot(range(0, len(data)), data, label=month)\n",
    "        plt.legend()\n",
    "        plt.suptitle(family)\n",
    "        plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:33.781570Z",
     "iopub.execute_input": "2022-12-14T01:58:33.782859Z",
     "iopub.status.idle": "2022-12-14T01:58:33.798885Z",
     "shell.execute_reply.started": "2022-12-14T01:58:33.782803Z",
     "shell.execute_reply": "2022-12-14T01:58:33.797518Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the sales over each week of the month\n",
    "if plot_timeseries:\n",
    "    weekly_df = avg_family_df.groupby(['week', 'dayofweek', 'family']).sales.mean().reset_index()\n",
    "    for family in weekly_df.family.unique():\n",
    "        for week in weekly_df.week.unique():\n",
    "            data = weekly_df.loc[(weekly_df.week == week) & (weekly_df.family == family), 'sales']\n",
    "            plt.plot(range(0, len(data)), data, label=week)\n",
    "        plt.suptitle(family)\n",
    "        plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:33.805430Z",
     "iopub.execute_input": "2022-12-14T01:58:33.806100Z",
     "iopub.status.idle": "2022-12-14T01:58:33.812873Z",
     "shell.execute_reply.started": "2022-12-14T01:58:33.806038Z",
     "shell.execute_reply": "2022-12-14T01:58:33.811931Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add time-related features\n",
    "\n",
    "# * add yearly \n",
    "feature_df['year'] = feature_df.index.get_level_values('date').year.astype('int')\n",
    "# * add quarterly trend\n",
    "feature_df['quarter'] = feature_df.index.get_level_values('date').quarter.astype('int')\n",
    "# * add daily trend\n",
    "feature_df['day'] = feature_df.index.get_level_values('date').day.astype('int')\n",
    "\n",
    "# * add weekly trend\n",
    "feature_df['weekofyear'] = feature_df.index.get_level_values('date').week.astype('int')\n",
    "# * add day of the week trend\n",
    "feature_df['dayofweek'] = feature_df.index.get_level_values('date').day_of_week.astype('int')\n",
    "# * add weekend trend\n",
    "feature_df['isweekend'] = feature_df.dayofweek.apply(lambda x: 1 if x in (5,6) else 0)\n",
    "\n",
    "# * add newyear day trend\n",
    "feature_df['newyear'] = feature_df.index.get_level_values('date').dayofyear == 1\n",
    "# * add school starting day trend\n",
    "feature_df['startschool'] = feature_df.index.get_level_values('date').month.isin((4,5,8,9))\n",
    "# * add weekend trend\n",
    "feature_df['weekend'] = feature_df.index.get_level_values('date').dayofweek.isin((5,6))\n",
    "\n",
    "# as wages in the public sector are paid every two weeks on the 15th and the last day of the month, add a trend for that too!\n",
    "feature_df['wageday'] = ((feature_df.index.get_level_values('date').to_timestamp().is_month_end) | (feature_df.index.get_level_values('date').day == 15))\n",
    "\n",
    "# add a trend for the days affected by earthquakes\n",
    "earthquake_affected_dates = [d.to_period('D') for d in pd.date_range('2016-04-16', periods=20, freq='D').tolist()]\n",
    "feature_df['earthquake_Aprin2016_effect'] = feature_df.index.get_level_values('date').isin(earthquake_affected_dates)\n",
    "\n",
    "# add some dummy features\n",
    "feature_df = pd.get_dummies(feature_df, columns=['year'], drop_first=True)\n",
    "feature_df = pd.get_dummies(feature_df, columns=['quarter'], drop_first=True)\n",
    "feature_df = pd.get_dummies(feature_df, columns=['dayofweek'], drop_first=True)\n",
    "\n",
    "# add trend for weekly on promotion\n",
    "feature_df['onpromotion_week_avg'] = feature_df['onpromotion'].rolling(7).mean()\n",
    "# add trend for biweekly on promotion\n",
    "feature_df['onpromotion_biweek_avg'] = feature_df['onpromotion'].rolling(14).mean()\n",
    "# add trend for monthly on promotion\n",
    "feature_df['onpromotion_1_month_avg'] = feature_df['onpromotion'].rolling(30).mean()\n",
    "# add trend for bimonthly on promotion\n",
    "feature_df['onpromotion_2_month_avg'] = feature_df['onpromotion'].rolling(60).mean()\n",
    "# drop columns 'city', 'cluster', 'state', 'store'\n",
    "feature_df.drop(columns=['city', 'cluster', 'state', 'store'], inplace=True)"
   ],
   "metadata": {
    "_uuid": "b6fb49b1-12ab-422f-809c-d37fa04ce154",
    "_cell_guid": "a019fa97-c68a-421f-ba5e-64e3a679aa96",
    "collapsed": false,
    "papermill": {
     "duration": 9.758802,
     "end_time": "2022-11-26T04:31:19.263974",
     "exception": false,
     "start_time": "2022-11-26T04:31:09.505172",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:33.814538Z",
     "iopub.execute_input": "2022-12-14T01:58:33.814922Z",
     "iopub.status.idle": "2022-12-14T01:58:43.536715Z",
     "shell.execute_reply.started": "2022-12-14T01:58:33.814887Z",
     "shell.execute_reply": "2022-12-14T01:58:43.535354Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot periodograms of the data to look at seasonality in the data\n",
    "# Code from Kaggle Timeseries Course: https://www.kaggle.com/code/ryanholbrook/seasonality\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n",
    "    frequencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(frequencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=90,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:43.538402Z",
     "iopub.execute_input": "2022-12-14T01:58:43.538748Z",
     "iopub.status.idle": "2022-12-14T01:58:43.548035Z",
     "shell.execute_reply.started": "2022-12-14T01:58:43.538717Z",
     "shell.execute_reply": "2022-12-14T01:58:43.546787Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the periodograms per family\n",
    "if plot_timeseries:\n",
    "    for family in avg_family_df.family.unique():\n",
    "        ax = plot_periodogram(avg_family_df.loc[avg_family_df.family==family, 'sales'])\n",
    "        plt.suptitle(family)\n",
    "        plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:43.549438Z",
     "iopub.execute_input": "2022-12-14T01:58:43.549906Z",
     "iopub.status.idle": "2022-12-14T01:58:43.568187Z",
     "shell.execute_reply.started": "2022-12-14T01:58:43.549869Z",
     "shell.execute_reply": "2022-12-14T01:58:43.566900Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Look at lag information in the dataset\n",
    "if plot_timeseries:\n",
    "    from statsmodels.graphics.tsaplots import plot_pacf\n",
    "    for family in avg_family_df.family.unique():\n",
    "        plot_pacf(avg_family_df.loc[avg_family_df.family==family, 'sales'], lags=14, title=family)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:43.569416Z",
     "iopub.execute_input": "2022-12-14T01:58:43.570282Z",
     "iopub.status.idle": "2022-12-14T01:58:43.581087Z",
     "shell.execute_reply.started": "2022-12-14T01:58:43.570122Z",
     "shell.execute_reply": "2022-12-14T01:58:43.579912Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# add deterministic time-series features\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "\n",
    "# calculate fourier transforms in different frequency domains\n",
    "fourierA = CalendarFourier(freq='A', order=5)\n",
    "fourierQ = CalendarFourier(freq='Q', order=3) # added quarterly frequency\n",
    "fourierM = CalendarFourier(freq='M', order=2)\n",
    "fourierW = CalendarFourier(freq='W', order=3)\n",
    "fourierB = CalendarFourier(freq='B', order=2) # added business day frequency\n",
    "\n",
    "# instance of DeterministicProcess\n",
    "dp = DeterministicProcess(index=feature_df.index.get_level_values('date'),\n",
    "                          order=1,\n",
    "                          seasonal=True,\n",
    "                          constant=False,\n",
    "                          additional_terms=[fourierA, fourierQ, fourierM, fourierW, fourierB],\n",
    "                          drop=True)\n",
    "\n",
    "dp_df = dp.in_sample()\n",
    "# concatenate these deterministic features with feature dataframe\n",
    "feature_df = pd.concat([feature_df.reset_index(level=['store_nbr', 'family']), dp_df], axis=1)\n",
    "\n",
    "# set the indices again\n",
    "feature_df = feature_df.set_index(['store_nbr', 'family'], append=True)"
   ],
   "metadata": {
    "_uuid": "0fd45856-325e-435e-94e0-aa1b27b6d31a",
    "_cell_guid": "10532692-832e-4b11-8175-0cffbe6a8e99",
    "collapsed": false,
    "papermill": {
     "duration": 13.592494,
     "end_time": "2022-11-26T04:31:32.90713",
     "exception": false,
     "start_time": "2022-11-26T04:31:19.314636",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:58:43.582514Z",
     "iopub.execute_input": "2022-12-14T01:58:43.582964Z",
     "iopub.status.idle": "2022-12-14T01:59:07.678715Z",
     "shell.execute_reply.started": "2022-12-14T01:58:43.582933Z",
     "shell.execute_reply": "2022-12-14T01:59:07.677663Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# helper methods that will be used later-on in the code\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "def _scorer(y_true, y_predicted):\n",
    "    y_true = np.exp(y_true)-1\n",
    "    y_predicted = np.exp(y_predicted)-1\n",
    "    return mean_squared_log_error(y_true, y_predicted, squared=False)\n",
    "\n",
    "scorer = make_scorer(_scorer, greater_is_better=False)\n",
    "\n",
    "def index_timeseries_train_test_split(df, date_index=False, date_format='%Y-%m-%d', earliest_train_date=None, split_date=None, last_test_date=None, verbose=True):\n",
    "    \"\"\"\n",
    "    =======\n",
    "    Example\n",
    "    =======\n",
    "    * earliest_train_date: 2020-02-01\n",
    "    * split_date: 2020-04-01\n",
    "    * last_test_date: 2020-07-01\n",
    "\n",
    "    Timeline:\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "    dates --> 2020-01-01 ... 2020-02-01 ... 2020-03-01 ... 2020-04-01 .. 2020-07-01 ... 2020-10-01 ...\n",
    "                                 /\\                          /\\             /\\\n",
    "                                 |                           |              |\n",
    "                        earliest_train_date             split_date     last_test_date\n",
    "    ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "    Finallly:\n",
    "    ---------\n",
    "    2020-02-01 <= TRAIN DATASET DATES < 2020-04-01\n",
    "    2020-04-01 <= TEST DATASET DATES < 2020-07-01                              \n",
    "    \"\"\"\n",
    "    dates = df.index.get_level_values('date').to_timestamp()\n",
    "\n",
    "    if earliest_train_date:\n",
    "        earliest_train_datetime = datetime.strptime(earliest_train_date, date_format)\n",
    "        df = df.loc[[earliest_train_datetime<=date for date in dates]]\n",
    "        dates = df.index.get_level_values('date').to_timestamp()\n",
    "\n",
    "    if last_test_date:\n",
    "        last_test_datetime = datetime.strptime(last_test_date, date_format)\n",
    "        df = df.loc[[date<last_test_datetime for date in dates]]\n",
    "        dates = df.index.get_level_values('date').to_timestamp()\n",
    "\n",
    "    split_datetime = datetime.strptime(split_date, date_format)\n",
    "    train_dates = [date<split_datetime for date in dates]\n",
    "    test_dates = np.invert(train_dates)\n",
    "\n",
    "    train_df = df.loc[train_dates]\n",
    "    test_df =  df.loc[test_dates]\n",
    "\n",
    "    if verbose:\n",
    "        print('================================')\n",
    "        print('Training Dataset last rows')\n",
    "        ICD.display(train_df.tail(3))\n",
    "        print('================================')\n",
    "        print('Test Dataset first rows')\n",
    "        ICD.display(test_df.head(3))\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def separate_X_and_y(df, target='sales'):\n",
    "    y = df[target].values\n",
    "    X = df.drop(columns=target)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def unstack_transformation(X, y, group_by='date', target='sales', levels=['store_nbr', 'family']):\n",
    "    df = X.copy()\n",
    "    df[target] = y\n",
    "    y = df.loc[:, target].unstack(levels)\n",
    "    X = df.drop(columns=[target]).groupby(by=group_by).first()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_sample_weights(X, target_date, weight=0.9):\n",
    "    extra_weight_days = X.index.get_level_values('date') > target_date\n",
    "    return np.array(list(map(lambda x: np.exp(-weight) if x == 0 else 1, extra_weight_days.astype('int'))))\n",
    "\n",
    "\n",
    "def get_closed_stores(df, last_days=15):\n",
    "    closed_stores_df = df.groupby(['family']).tail(last_days).groupby(['family'])['sales'].sum().reset_index()\n",
    "    closed_stores_df = closed_stores_df[closed_stores_df['sales'] == 0].drop('sales', axis = 1)\n",
    "    closed_stores_df = closed_stores_df[closed_stores_df['family'] != \"SCHOOL AND OFFICE SUPPLIES\"]\n",
    "    closed_stores_df = closed_stores_df.set_index(['family'])\n",
    "    closed_stores_df['isclosed'] = True\n",
    "    return closed_stores_df\n",
    "\n",
    "\n",
    "def apply_zero_forecasting(df, closed_stores_df):\n",
    "    df = df.merge(closed_stores_df, how='left',  left_index=True, right_index=True)\n",
    "    df.loc[df['isclosed']==True, 'sales']=0\n",
    "    df.drop(columns=['isclosed'], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def timeit(function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        ts = time.perf_counter()\n",
    "        func = function(*args, **kwargs)\n",
    "        te = time.perf_counter ()\n",
    "        run_time = time.perf_counter() - ts\n",
    "        print(f'Run Time: {run_time} secs')\n",
    "        return func\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_importances, feature_names):\n",
    "    feats = {} # a dict to hold feature_name: feature_importance\n",
    "    for feature, importance in zip(feature_names, feature_importances):\n",
    "        feats[feature] = importance #add the name/value pair \n",
    "\n",
    "    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "    importances.sort_values(by='Gini-importance').plot(kind='bar').show()\n",
    "\n",
    "\n",
    "def plot_feature_elimination_score(min_features_to_select, grid_score):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Number of features selected')\n",
    "    plt.ylabel('Cross validation score')\n",
    "    plt.plot(\n",
    "        range(min_features_to_select, len(grid_score) + min_features_to_select),\n",
    "        grid_score\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predition_timeseries(x, y_actual, y_pred):\n",
    "    df = pd.DataFrame()\n",
    "    df['date'], df['actual'], df['prediction'] = x, y_actual, y_pred\n",
    "    df = df.groupby(['date'], as_index=False).sum().reset_index()\n",
    "    df['date'] = df['date'].apply(lambda row: row.to_timestamp())\n",
    "    title = 'Actual vs Prediction'\n",
    "    df.plot.line(x='date', y=['actual', 'prediction'], title=title).show()\n",
    "\n",
    "\n",
    "def transform_prediction_to_df(X, y, prediction):\n",
    "    return pd.DataFrame(pd.DataFrame(prediction,\n",
    "                        index=X.index,\n",
    "                        columns=y.columns).stack(['store_nbr'])).rename(columns={0: 'sales'})\n",
    "\n",
    "\n",
    "def per_family_df_generator(train_df, test_df, verbose=False):\n",
    "    families = sorted(train_df.index.get_level_values('family').unique())\n",
    "    for _, family in zip(tqdm(range(len(families))), families):\n",
    "        if verbose:\n",
    "            print('='*40)\n",
    "            print(f'Family: {family}')\n",
    "        # Train DF   \n",
    "        train_family_df = train_df[train_df.index.get_level_values('family')==family]\n",
    "        train_family_df.index = train_family_df.index.droplevel('family')\n",
    "            \n",
    "        # Test DF \n",
    "        test_family_df = test_df[test_df.index.get_level_values('family')==family]\n",
    "        test_family_df.index = test_family_df.index.droplevel('family')\n",
    "        yield family, train_family_df, test_family_df"
   ],
   "metadata": {
    "_uuid": "930f6d04-8869-4642-ba02-7e06849fceba",
    "_cell_guid": "8243d9eb-7b11-41c0-96b8-7ef8b31ca914",
    "collapsed": false,
    "papermill": {
     "duration": 1.588482,
     "end_time": "2022-11-26T04:31:34.613885",
     "exception": false,
     "start_time": "2022-11-26T04:31:33.025403",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:59:07.680537Z",
     "iopub.execute_input": "2022-12-14T01:59:07.680915Z",
     "iopub.status.idle": "2022-12-14T01:59:08.265302Z",
     "shell.execute_reply.started": "2022-12-14T01:59:07.680881Z",
     "shell.execute_reply": "2022-12-14T01:59:08.264137Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define estimators, metrics, ensemble and standalone models\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.ensemble import BaggingRegressor, ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "class MultiOutBaggingExtraTreesRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                extra_trees_n_estimators=200,\n",
    "                bagging_n_estimators=10,\n",
    "                log_target_transform=True,\n",
    "                multioutput=True,\n",
    "                n_jobs=-1,\n",
    "                random_state=42):\n",
    "        super().__init__()\n",
    "        _r1 = ExtraTreesRegressor(n_estimators=extra_trees_n_estimators,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "        _estimator = BaggingRegressor(base_estimator=_r1,\n",
    "                                      n_estimators=bagging_n_estimators,\n",
    "                                      bootstrap=True,\n",
    "                                      n_jobs=n_jobs,\n",
    "                                      random_state=random_state)\n",
    "        if multioutput:\n",
    "            _estimator = MultiOutputRegressor(_estimator)\n",
    "        self.estimator_ = _estimator\n",
    "        self.log_target_transform = log_target_transform\n",
    "        self.feature_importances_ = None \n",
    "        self.bagging_n_estimators = bagging_n_estimators\n",
    "        self.extra_trees_n_estimators = extra_trees_n_estimators\n",
    "        self.multioutput = multioutput\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if self.log_target_transform:\n",
    "            y = np.log1p(y)\n",
    "        self.estimator_.fit(X, y, sample_weight)\n",
    "        \n",
    "        if self.multioutput:\n",
    "            feature_importances_ = []\n",
    "            for n_output_estimators in self.estimator_.estimators_:\n",
    "                feature_importances_.append(np.mean([\n",
    "                tree.feature_importances_ for tree in n_output_estimators.estimators_], axis=0))\n",
    "            self.feature_importances_ = np.mean(feature_importances_, axis=0)   \n",
    "        else:\n",
    "            self.feature_importances_ = np.mean([\n",
    "                tree.feature_importances_ for tree in self.estimator_.estimators_], axis=0)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        prediction = self.estimator_.predict(X)\n",
    "        if self.log_target_transform:\n",
    "            prediction = (np.exp(prediction)-1)\n",
    "        return prediction.clip(0)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X, y)\n",
    "        return mean_squared_log_error(y, prediction, squared=False)"
   ],
   "metadata": {
    "_uuid": "3c949dd2-9d3a-4809-808b-3f6fd5267367",
    "_cell_guid": "c67c281f-12f4-4add-a93d-80dae487cb8f",
    "collapsed": false,
    "papermill": {
     "duration": 0.274057,
     "end_time": "2022-11-26T04:31:34.941086",
     "exception": false,
     "start_time": "2022-11-26T04:31:34.667029",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:59:08.266621Z",
     "iopub.execute_input": "2022-12-14T01:59:08.266961Z",
     "iopub.status.idle": "2022-12-14T01:59:08.513232Z",
     "shell.execute_reply.started": "2022-12-14T01:59:08.266931Z",
     "shell.execute_reply": "2022-12-14T01:59:08.512257Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# method to train one estimator per family and generate predictions\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def family_prediction_generator(train_df, test_df, estimator_params={}, verbose=True):\n",
    "    for family, train_family_df, test_family_df in per_family_df_generator(train_df, test_df, verbose=verbose):\n",
    "        # generate multi-output (with unstack)\n",
    "        X_train, y_train = separate_X_and_y(train_family_df)\n",
    "        X_train, y_train = unstack_transformation(X_train, y_train, levels=['store_nbr'])\n",
    "        X_test, y_test = separate_X_and_y(test_family_df)\n",
    "        X_test, y_test = unstack_transformation(X_test, y_test, levels=['store_nbr'])\n",
    "\n",
    "        # perform training\n",
    "        no_multiple_outputs = ['BABY CARE', 'GROCERY II','HOME AND KITCHEN I','HOME AND KITCHEN II', 'SCHOOL AND OFFICE SUPPLIES']\n",
    "        if family in no_multiple_outputs:\n",
    "            estimator = MultiOutBaggingExtraTreesRegressor(**estimator_params, multioutput=False)\n",
    "        else:\n",
    "            estimator = MultiOutBaggingExtraTreesRegressor(**estimator_params)\n",
    "            \n",
    "        weights = create_sample_weights(X_train, '2017-07-01')    \n",
    "        estimator.fit(X_train, y_train, sample_weight=weights)\n",
    "        \n",
    "        if plot_importances:\n",
    "            feature_importances = estimator.feature_importances_\n",
    "            feature_names = X_train.columns\n",
    "            plot_feature_importance(feature_importances, feature_names)\n",
    "\n",
    "        # generate predictions for training & test sets\n",
    "        train_pred = estimator.predict(X_train)\n",
    "        train_pred_df = transform_prediction_to_df(X_train, y_train, train_pred)\n",
    "        train_pred_df['family'] = family\n",
    "        train_pred_df = train_pred_df.set_index('family', append=True)\n",
    "\n",
    "        test_pred = estimator.predict(X_test)\n",
    "        test_pred_df = transform_prediction_to_df(X_test, y_test, test_pred)\n",
    "        test_pred_df['family'] = family\n",
    "        test_pred_df = test_pred_df.set_index('family', append=True)\n",
    "\n",
    "        # score the predictions\n",
    "        family_train_score = mean_squared_log_error(train_family_df['sales'].values,\n",
    "                                                    train_pred_df['sales'].values,\n",
    "                                                    squared=False)\n",
    "        if verbose:\n",
    "            print(f'RMSLE || TRAINING Score: {family_train_score}')\n",
    "            \n",
    "        if not any(np.isnan(test_family_df['sales'].values)):\n",
    "            # Print score only in evaluation\n",
    "            family_test_score = mean_squared_log_error(test_family_df['sales'].values,\n",
    "                                                       test_pred_df['sales'].values,\n",
    "                                                       squared=False)\n",
    "            if verbose:\n",
    "                print(f'RMSLE || TEST Score: {family_test_score}')  \n",
    "        yield train_pred_df, test_pred_df"
   ],
   "metadata": {
    "_uuid": "24c3edfb-c15f-4491-85ad-2627dadc558e",
    "_cell_guid": "a006f872-b3dd-44d2-b513-792910953cbc",
    "collapsed": false,
    "papermill": {
     "duration": 0.036511,
     "end_time": "2022-11-26T04:31:35.029693",
     "exception": false,
     "start_time": "2022-11-26T04:31:34.993182",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:59:08.514749Z",
     "iopub.execute_input": "2022-12-14T01:59:08.515210Z",
     "iopub.status.idle": "2022-12-14T01:59:08.529137Z",
     "shell.execute_reply.started": "2022-12-14T01:59:08.515178Z",
     "shell.execute_reply": "2022-12-14T01:59:08.528032Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# generate train and test dataframes\n",
    "train_df, test_df = index_timeseries_train_test_split(df=feature_df,\n",
    "                                                      date_index='date',\n",
    "                                                      date_format='%Y-%m-%d',\n",
    "                                                      earliest_train_date='2013-03-01',\n",
    "                                                      split_date='2017-07-15', # '2016-08-01',\n",
    "                                                      last_test_date='2017-08-01',  #'2016-09-15',\n",
    "                                                      verbose=False)"
   ],
   "metadata": {
    "_uuid": "49376659-0797-4cae-8833-81e5d644e011",
    "_cell_guid": "5d9b9c13-7c74-4a3d-9378-cd34d50608e7",
    "collapsed": false,
    "papermill": {
     "duration": 30.149612,
     "end_time": "2022-11-26T04:32:05.233672",
     "exception": false,
     "start_time": "2022-11-26T04:31:35.08406",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:59:08.530359Z",
     "iopub.execute_input": "2022-12-14T01:59:08.531248Z",
     "iopub.status.idle": "2022-12-14T01:59:39.668570Z",
     "shell.execute_reply.started": "2022-12-14T01:59:08.531212Z",
     "shell.execute_reply": "2022-12-14T01:59:39.667182Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# train one estimator per family and generate predictions\n",
    "estimator_params = {\n",
    "                    'extra_trees_n_estimators': 5,\n",
    "                    'bagging_n_estimators': 5,\n",
    "    \n",
    "    }\n",
    "\n",
    "training_total_pred_df = pd.DataFrame()\n",
    "test_total_pred_df = pd.DataFrame()\n",
    "for train_pred_df, test_pred_df in family_prediction_generator(train_df, test_df, estimator_params):\n",
    "    # retain predictions for later plotting\n",
    "    training_total_pred_df = pd.concat([training_total_pred_df, train_pred_df])\n",
    "    test_total_pred_df = pd.concat([test_total_pred_df, test_pred_df])"
   ],
   "metadata": {
    "_uuid": "a4bc6db6-1f39-4361-8cce-61b6b3e6a240",
    "_cell_guid": "1e455d98-3d4d-4a2b-9c7e-b7747219982b",
    "collapsed": false,
    "papermill": {
     "duration": 2781.625871,
     "end_time": "2022-11-26T05:18:26.87792",
     "exception": false,
     "start_time": "2022-11-26T04:32:05.252049",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T01:59:39.670007Z",
     "iopub.execute_input": "2022-12-14T01:59:39.670423Z",
     "iopub.status.idle": "2022-12-14T02:32:37.923205Z",
     "shell.execute_reply.started": "2022-12-14T01:59:39.670386Z",
     "shell.execute_reply": "2022-12-14T02:32:37.921839Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print('===> Print All stores Training Prediction')\n",
    "training_total_pred_df = training_total_pred_df.sort_values(['date', 'store_nbr', 'family'])\n",
    "training_score = mean_squared_log_error(train_df['sales'].values, training_total_pred_df['sales'].values, squared=False)\n",
    "plot_predition_timeseries(x=training_total_pred_df.index.get_level_values('date'), \n",
    "                            y_actual=train_df['sales'].values,\n",
    "                            y_pred=training_total_pred_df['sales'].values)\n",
    "\n",
    "print('===> Print All stores Test Prediction')\n",
    "test_total_pred_df = test_total_pred_df.sort_values(['date', 'store_nbr', 'family'])\n",
    "test_score = mean_squared_log_error(test_df['sales'].values, test_total_pred_df['sales'].values, squared=False)\n",
    "plot_predition_timeseries(x=test_total_pred_df.index.get_level_values('date'), \n",
    "                            y_actual=test_df['sales'].values,\n",
    "                            y_pred=test_total_pred_df['sales'].values)\n",
    "\n",
    "print(f'RMSLE || TRAINING Score: {training_score} | TEST Score: {test_score}')"
   ],
   "metadata": {
    "_uuid": "b035d507-adb3-4d49-832a-0cac974ce8a1",
    "_cell_guid": "d13abdc8-c597-471d-aa1b-0bdc009ac720",
    "collapsed": false,
    "papermill": {
     "duration": 4.823383,
     "end_time": "2022-11-26T05:18:32.027371",
     "exception": false,
     "start_time": "2022-11-26T05:18:27.203988",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T02:32:37.924862Z",
     "iopub.execute_input": "2022-12-14T02:32:37.926051Z",
     "iopub.status.idle": "2022-12-14T02:32:42.729092Z",
     "shell.execute_reply.started": "2022-12-14T02:32:37.926007Z",
     "shell.execute_reply": "2022-12-14T02:32:42.727905Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# create the submission dataframe\n",
    "train_df, test_df = index_timeseries_train_test_split(df=feature_df,\n",
    "                                                      date_index='date',\n",
    "                                                      date_format='%Y-%m-%d',\n",
    "                                                      earliest_train_date='2013-03-01',\n",
    "                                                      split_date='2017-08-16',\n",
    "                                                      verbose=True)\n",
    "\n",
    "# get stopped families for each store\n",
    "closed_stores_df = get_closed_stores(train_df, last_days=12)\n",
    "\n",
    "\n",
    "training_total_pred_df = pd.DataFrame()\n",
    "test_total_pred_df = pd.DataFrame()\n",
    "for train_pred_df, test_pred_df in family_prediction_generator(train_df, test_df):\n",
    "    # retain prediction for submission\n",
    "    training_total_pred_df = pd.concat([training_total_pred_df, train_pred_df])\n",
    "    test_total_pred_df = pd.concat([test_total_pred_df, test_pred_df])"
   ],
   "metadata": {
    "_uuid": "ca48b937-6fd5-492b-bfa1-9d29cbff474b",
    "_cell_guid": "1a437d07-420a-4243-a806-378eeaf3021f",
    "collapsed": false,
    "papermill": {
     "duration": 22899.386966,
     "end_time": "2022-11-26T11:40:12.752086",
     "exception": false,
     "start_time": "2022-11-26T05:18:33.36512",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T02:32:42.730540Z",
     "iopub.execute_input": "2022-12-14T02:32:42.730888Z",
     "iopub.status.idle": "2022-12-14T08:41:00.970795Z",
     "shell.execute_reply.started": "2022-12-14T02:32:42.730857Z",
     "shell.execute_reply": "2022-12-14T08:41:00.969528Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_total_pred_df = test_total_pred_df.sort_values(['date', 'store_nbr', 'family'])\n",
    "\n",
    "# Zero the sales for stopped families\n",
    "test_total_pred_df= apply_zero_forecasting(test_total_pred_df, closed_stores_df)\n",
    "\n",
    "ids = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv')['id']\n",
    "test_total_pred_df['id'] = ids.values\n",
    "\n",
    "submit_df = test_total_pred_df[['id', 'sales']]\n",
    "\n",
    "if len(submit_df) != 28512: # Minor check before submission :-)\n",
    "    raise Exception(f'The rows ({len(submit_df)}) of the export are not correct')\n",
    "else:\n",
    "    current_dateTime = datetime.now()\n",
    "    latest_filename = 'submission.csv'\n",
    "\n",
    "    submit_df.to_csv(latest_filename, index=False)\n",
    "    print('Submition created successfully!!')"
   ],
   "metadata": {
    "_uuid": "c11babe9-ad36-46f9-8828-d31dfb8e3532",
    "_cell_guid": "84985bac-574a-4c14-a833-809bb666db2c",
    "collapsed": false,
    "papermill": {
     "duration": 0.986573,
     "end_time": "2022-11-26T11:40:15.593519",
     "exception": false,
     "start_time": "2022-11-26T11:40:14.606946",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-12-14T08:41:00.973044Z",
     "iopub.execute_input": "2022-12-14T08:41:00.973476Z",
     "iopub.status.idle": "2022-12-14T08:41:01.126627Z",
     "shell.execute_reply.started": "2022-12-14T08:41:00.973440Z",
     "shell.execute_reply": "2022-12-14T08:41:01.125240Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
